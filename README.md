# Fed-NAD-Backdoor-Resilient-Federated-Learning-via-Neural-Attention-Distillation
Fed-NAD implements a backdoor attack mitigation approach for Federated Learning using Neural Attention Distillation (NAD). It enhances model resilience by filtering out malicious updates during training, ensuring secure aggregation without compromising performance. Easy to integrate, scalable, and open-source.
